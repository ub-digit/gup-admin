{"data": {"id": "scopus_85165749972", "publication_type_id": 2, "publication_type_label": "Paper i proceeding", "title": "New or Old? Exploring How Pre-Trained Language Models Represent Discourse Entities", "pubyear": "2022", "sourcetitle": "Proceedings - International Conference on Computational Linguistics, COLING", "issn": "29512093", "eissn": null, "sourcevolume": "29", "sourceissue": "1", "sourcepages": "875-886", "articlenumber": null, "abstract": "Recent research shows that pre-trained language models, built to generate text conditioned on some context, learn to encode syntactic knowledge to a certain degree. This has motivated researchers to move beyond the sentence-level and look into their ability to encode less studied discourse-level phenomena. In this paper, we add to the body of probing research by investigating discourse entity representations in large pre-trained language models in English. Motivated by early theories of discourse and key pieces of previous work, we focus on the information-status of entities as discourse-new or discourse-old. We present two probing models, one based on binary classification and another one on sequence labeling. The results of our experiments show that pretrained language models do encode information on whether an entity has been introduced before or not in the discourse. However, this information alone is not sufficient to find the entities in a discourse, opening up interesting questions about the definition of entities for future work.", "keywords": null, "publication_identifiers": [{"identifier_code": "scopus-id", "identifier_value": "85165749972"}], "source": "scopus", "attended": false}}